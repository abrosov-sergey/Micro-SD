**Dataset**
A collection of data, typically structured into records and fields, representing the entire set of information.

**Feature**
A distinct attribute or variable within a dataset, representing a specific type of information.

**Value**
The specific data associated with a feature for an individual record.

**Record**
A single entry in a dataset, comprising a set of values across multiple features. 

**Unique Identifier**
A feature or combination of features that uniquely identify a record within a dataset.

**Category**
A type of feature used to group similar data points.

**Null Value**
An empty or missing value in a dataset.

**ETL (Extract, Transform, Load) Service**
A system that extracts data from internal corporate sources, transforms it through cleansing and other processes, and loads it into data modules.

**VS Code Extension**
An add-on for Visual Studio Code that extends its functionality; in this project, the ETL service operates as such an extension.

**Anonymization**
The process of removing or altering personally identifiable information from data to protect individual privacy while maintaining data utility for analysis.

**Data Transformation**
Modifying data's format, structure, or values to meet specific requirements or prepare it for further analysis.

**Data Cleansing**
The multistage process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset to improve its quality and reliability.

**Data Masking**
A method of obscuring specific data within a dataset to protect sensitive information while allowing the dataset to remain usable for analysis.

**Data Merging**
Combining data from multiple sources into a single, unified dataset based on matching unique identifiers, resolving discrepancies as needed.

**JSON (JavaScript Object Notation)**
A lightweight data interchange format using human-readable text to store and transmit data objects consisting of attributeâ€“value pairs.

**JSON-LD (JSON for Linked Data)**
A method of encoding linked data using JSON, enabling data to be serialized in a way that is easy to read and write by both humans and machines.

**Logging**
The practice of recording events, errors, and other significant occurrences during data processing for monitoring and debugging purposes.

**Metadata**
Data that provides information about other data, such as its origin, structure, and any transformations applied, stored in JSON-LD format.

**Normalization**
The process of standardizing data to a consistent format or scale, ensuring uniformity across the dataset and eliminating redundancies.

**Null Value Test**
A data quality test that identifies missing or null values in critical fields, flagging them for correction or handling according to predefined rules.

**Standardizing Categories**
The process of unifying similar categories with variations into a single, consistent format to ensure data uniformity.

**String Patterns Test**
A data quality test that checks whether string fields conform to specific patterns or formats, such as email addresses or phone numbers.

**Versioned Data**
Data that is managed through version control, allowing users to track changes, revert to previous versions, and maintain a history of data modifications.
